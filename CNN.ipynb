{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFT7_RqoHk1",
        "outputId": "71fbf6d8-871d-4772-8d30-b151f5a410ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from HodaDatasetReader import read_hoda_cdb, read_hoda_dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "2egVhb6yoJrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = read_hoda_dataset(dataset_path='Train 60000.cdb',\n",
        "                                images_height=32,\n",
        "                                images_width=32,\n",
        "                                one_hot=False,\n",
        "                                reshape=True)\n",
        " \n",
        "X_test, y_test = read_hoda_dataset(dataset_path='Test 20000.cdb',\n",
        "                              images_height=32,\n",
        "                              images_width=32,\n",
        "                              one_hot=False,\n",
        "                              reshape=True)\n",
        "\n",
        "X_remaining, y_remaining = read_hoda_dataset(dataset_path='RemainingSamples.cdb',\n",
        "                              images_height=32,\n",
        "                              images_width=32,\n",
        "                              one_hot=False,\n",
        "                              reshape=True)"
      ],
      "metadata": {
        "id": "bXNCAQUWt2ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "vlbBBlGSuHWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d4UYGXSuI5A",
        "outputId": "2265fd28-d2a8-4274-e5c7-5b908109d14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4c9710e4b0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(60000,32,32)\n",
        "X_test = X_test.reshape(20000,32,32)\n",
        "X_train.shape\n",
        "#X_test.shape\n",
        "#X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gLS_608uP7g",
        "outputId": "a4c18faa-7877-4164-f241-8a381adfad94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 32, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(9):\t\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    plt.imshow(X_train[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "slJ-CVqpuQzu",
        "outputId": "d7c546ca-2e0a-4e8f-c25d-72aaf1660722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
        "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) \n",
        "\n",
        "torch_X_train = torch_X_train.view(-1,1,32,32).float()\n",
        "torch_X_train.shape"
      ],
      "metadata": {
        "id": "XPiA0vapxu_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d4c002-c464-4561-f71d-625b140708d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 1, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
        "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
        "\n",
        "torch_X_test = torch_X_test.view(-1,1,32,32).float()"
      ],
      "metadata": {
        "id": "Oo_4IXWSxvSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\n",
        "test = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)"
      ],
      "metadata": {
        "id": "pGi3YqHBxvkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = Fulse)"
      ],
      "metadata": {
        "id": "m1_zPsjnCRQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(500, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 500)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "g14JgWa4CRbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN()\n",
        "print(cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS4UuopqCb6o",
        "outputId": "1bb8e8f4-57b1-43b6-94f7-6200f21ec538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    cnn = cnn.cuda()"
      ],
      "metadata": {
        "id": "TilprPliCcGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(train_loader)\n",
        "X_batch, y_batch = next(it)\n",
        "print(X_batch.shape)\n",
        "print(y_batch.shape)\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "    #X_batch = X_batch.cuda()\n",
        "    #y_batch = y_batch.cuda()\n",
        "    \n",
        "print(cnn.forward(X_batch).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeGmyb-PCcOl",
        "outputId": "49270a5a-3088-4d24-e075-6b6f36f31f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 1, 32, 32])\n",
            "torch.Size([256])\n",
            "torch.Size([256, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    if batch_idx == 0:\n",
        "        print(data.shape)\n",
        "        print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G13hU-9SCcWs",
        "outputId": "ccc88247-58b4-4e86-805b-906636afbdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 1, 32, 32])\n",
            "torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "n_epochs = 15\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "scheduler = StepLR(optimizer,step_size=1 , gamma=0.9)\n",
        "#optimizer = optim.SGD(cnn.parameters(), lr=learning_rate,momentum=momentum)"
      ],
      "metadata": {
        "id": "g5tMKhK2EBaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []"
      ],
      "metadata": {
        "id": "7mBWQPPWEBlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    cnn.train()\n",
        "    train_loss = 0\n",
        "    scheduler.step()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        output = cnn(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 25 == 0: #every 25 * batchsize sample we print results\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "      \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_counter.append(epoch)\n",
        "\n",
        "    torch.save(cnn.state_dict(), 'model.pth')\n",
        "    torch.save(cnn.state_dict(), 'optimizer.pth')"
      ],
      "metadata": {
        "id": "A1jHYnfZEBxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  cnn.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        output = cnn(data)\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "M0MEuTRnEqxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUoannE-Eq9E",
        "outputId": "335ce4da-154e-4500-ea86-e447f7ebe384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.037512\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.169066\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.221198\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.078384\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.131505\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.139803\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.148466\n",
            "Train Epoch: 1 [44800/60000 (74%)]\tLoss: 0.165092\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.147568\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.062715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0820, Accuracy: 19460/20000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.082326\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.075071\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.061890\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.102009\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.098965\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.058255\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.152646\n",
            "Train Epoch: 2 [44800/60000 (74%)]\tLoss: 0.082218\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.100554\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.094894\n",
            "\n",
            "Test set: Avg. loss: 0.0704, Accuracy: 19572/20000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.078988\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.125634\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.089002\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.059983\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.110667\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.087427\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.081967\n",
            "Train Epoch: 3 [44800/60000 (74%)]\tLoss: 0.113728\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.055400\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.078938\n",
            "\n",
            "Test set: Avg. loss: 0.0658, Accuracy: 19586/20000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.079188\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.071904\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.157425\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.056479\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.054241\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.068224\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.088035\n",
            "Train Epoch: 4 [44800/60000 (74%)]\tLoss: 0.077515\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.066192\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.121450\n",
            "\n",
            "Test set: Avg. loss: 0.0560, Accuracy: 19677/20000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.049170\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.043712\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.113867\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.057836\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.082038\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.060378\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.138334\n",
            "Train Epoch: 5 [44800/60000 (74%)]\tLoss: 0.107778\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.074989\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.071525\n",
            "\n",
            "Test set: Avg. loss: 0.0567, Accuracy: 19665/20000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.061700\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.059446\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.151812\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.075291\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.111037\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.074508\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.057044\n",
            "Train Epoch: 6 [44800/60000 (74%)]\tLoss: 0.036077\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.057767\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.040294\n",
            "\n",
            "Test set: Avg. loss: 0.0498, Accuracy: 19711/20000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.050110\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.048951\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.109563\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.037423\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.098063\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.079034\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.073111\n",
            "Train Epoch: 7 [44800/60000 (74%)]\tLoss: 0.082212\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.067409\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.022835\n",
            "\n",
            "Test set: Avg. loss: 0.0482, Accuracy: 19706/20000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.090790\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.102334\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.097998\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.043293\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.051005\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.072947\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.102558\n",
            "Train Epoch: 8 [44800/60000 (74%)]\tLoss: 0.054501\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.050744\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.051472\n",
            "\n",
            "Test set: Avg. loss: 0.0549, Accuracy: 19660/20000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.030245\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.043581\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.094697\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.048663\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.042828\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.025102\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.090414\n",
            "Train Epoch: 9 [44800/60000 (74%)]\tLoss: 0.052334\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.026094\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.038215\n",
            "\n",
            "Test set: Avg. loss: 0.0469, Accuracy: 19711/20000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.053831\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.060972\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.062400\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.057541\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.071306\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.068896\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.060254\n",
            "Train Epoch: 10 [44800/60000 (74%)]\tLoss: 0.078663\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.057247\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.019732\n",
            "\n",
            "Test set: Avg. loss: 0.0517, Accuracy: 19711/20000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.044678\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.066670\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.078592\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.027594\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.032922\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.045734\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.062526\n",
            "Train Epoch: 11 [44800/60000 (74%)]\tLoss: 0.082243\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.041292\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.050013\n",
            "\n",
            "Test set: Avg. loss: 0.0433, Accuracy: 19748/20000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.042547\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.065009\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.089873\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.019972\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.020623\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.084851\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.047650\n",
            "Train Epoch: 12 [44800/60000 (74%)]\tLoss: 0.053709\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.039490\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.055358\n",
            "\n",
            "Test set: Avg. loss: 0.0450, Accuracy: 19766/20000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.023180\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.053779\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.052703\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.028946\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.026264\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.051214\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.086800\n",
            "Train Epoch: 13 [44800/60000 (74%)]\tLoss: 0.038719\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.011547\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.055588\n",
            "\n",
            "Test set: Avg. loss: 0.0490, Accuracy: 19742/20000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.064021\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.045419\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.057495\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.042468\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.054901\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.046518\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.047064\n",
            "Train Epoch: 14 [44800/60000 (74%)]\tLoss: 0.051572\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.031625\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.034640\n",
            "\n",
            "Test set: Avg. loss: 0.0454, Accuracy: 19756/20000 (99%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.029465\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.053213\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.068840\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.043236\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.023952\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.041216\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.047588\n",
            "Train Epoch: 15 [44800/60000 (74%)]\tLoss: 0.039679\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.043690\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.031674\n",
            "\n",
            "Test set: Avg. loss: 0.0438, Accuracy: 19766/20000 (99%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}